{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y52N8qzuVPLa"
   },
   "outputs": [],
   "source": [
    "# Instala y actualiza silenciosamente langchain, langchain-community y langchain-chroma\n",
    "%pip install --quiet --upgrade langchain langchain-community langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drMU-8LbVl5J"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_CqW0oHYVn7d",
    "outputId": "42a1b5ed-a0df-4d2d-8728-34f2f4060511"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.2 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
     ]
    }
   ],
   "source": [
    "# Configura la clave de API de OpenAI para acceder a sus modelos\n",
    "%pip install -qU langchain-openai\n",
    "# Instala BeautifulSoup para analizar HTML\n",
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8EmSP-dVrv7"
   },
   "outputs": [],
   "source": [
    "# Instancia del modelo ChatOpenAI GPT-4 mini\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "ZUjD2ux2Vz_V",
    "outputId": "6437196f-85b4-44ad-d5f2-7456da14f360"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=5b11a99f-dfcb-4a0d-bc7f-4a90fbb8d08a,id=5b11a99f-dfcb-4a0d-bc7f-4a90fbb8d08a; trace=5b11a99f-dfcb-4a0d-bc7f-4a90fbb8d08a,id=96bad104-24b1-4485-ad96-b807d834ae0f; trace=5b11a99f-dfcb-4a0d-bc7f-4a90fbb8d08a,id=c94759b6-83a6-49f0-a25c-429c619f50c4; trace=5b11a99f-dfcb-4a0d-bc7f-4a90fbb8d08a,id=a6cd35d1-3776-4b28-8dbb-6bde62e7ee34; trace=5b11a99f-dfcb-4a0d-bc7f-4a90fbb8d08a,id=6937db06-0d72-4fcc-b92e-511aae3c3360\n",
      "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=5b11a99f-dfcb-4a0d-bc7f-4a90fbb8d08a,id=742390ea-3f6f-447c-a2cd-4d71214a5891; trace=5b11a99f-dfcb-4a0d-bc7f-4a90fbb8d08a,id=19b7dd08-9868-40a0-b606-66cb0ebb6096; trace=5b11a99f-dfcb-4a0d-bc7f-4a90fbb8d08a,id=07d0b5b1-30f5-4fa0-abc5-a45a5602901a; trace=5b11a99f-dfcb-4a0d-bc7f-4a90fbb8d08a,id=c94759b6-83a6-49f0-a25c-429c619f50c4; trace=5b11a99f-dfcb-4a0d-bc7f-4a90fbb8d08a,id=a6cd35d1-3776-4b28-8dbb-6bde62e7ee34; trace=5b11a99f-dfcb-4a0d-bc7f-4a90fbb8d08a,id=96bad104-24b1-4485-ad96-b807d834ae0f\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Task Decomposition is the process of breaking down complex tasks into smaller, manageable steps to facilitate easier execution and understanding. This can be achieved through techniques like Chain of Thought (CoT) prompting, where models are guided to think step-by-step, or by using specific instructions and human inputs. Essentially, it transforms large tasks into simpler sub-tasks, enhancing the model's performance and interpretability.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=5b11a99f-dfcb-4a0d-bc7f-4a90fbb8d08a,id=cc4465c0-70b5-43ca-8788-4b1dff58fbff; trace=5b11a99f-dfcb-4a0d-bc7f-4a90fbb8d08a,id=5b11a99f-dfcb-4a0d-bc7f-4a90fbb8d08a; trace=5b11a99f-dfcb-4a0d-bc7f-4a90fbb8d08a,id=07d0b5b1-30f5-4fa0-abc5-a45a5602901a\n"
     ]
    }
   ],
   "source": [
    "# Carga y divide el contenido de un blog en fragmentos para indexación\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Carga contenido web y extrae solo elementos relevantes\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Divide el documento en fragmentos de 1000 caracteres con superposición de 200\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Crea un vector store e indexa los fragmentos\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Configura la recuperación y generación usando los fragmentos indexados\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Formatea los fragmentos recuperados\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Configura la cadena de recuperación y generación (RAG) para responder preguntas\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# Ejecuta una pregunta de ejemplo usando la cadena RAG\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTQfylHDV5kv"
   },
   "outputs": [],
   "source": [
    "# en la base de datos de vectores limpia y elimina la colección\n",
    "vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JZLDLOlUV-K8",
    "outputId": "739f0996-7962-47b6-d08c-d143fd5eb7e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43131"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extrae datos específicos de la página web\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Conserve únicamente el título de la publicación, los encabezados y el contenido del HTML completo.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Muestra la longitud del contenido\n",
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RSJ4bBrcV_LJ",
    "outputId": "2eac6ade-89f1-4d9e-c3e3-d3058c6d878c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "# Imprime los primeros 500 caracteres del contenido cargado para verificar\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SKh4LxwpWChY",
    "outputId": "8fc986ac-629a-4d57-e8af-55accea6d760"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configura un divisor de texto que incluye índices de inicio\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Muestra la cantidad de fragmentos\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bGnidlu3WDsu",
    "outputId": "c797ea66-1e1f-4b68-af6d-104c98d42065"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "969"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra la longitud del contenido del primer fragmento\n",
    "len(all_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQbP0Xy7WF-2",
    "outputId": "ead820b5-87a6-4990-ac67-6aa827b32963"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'start_index': 7056}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra los metadatos del décimo fragmento\n",
    "all_splits[10].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUc4wcQ1WG6A"
   },
   "outputs": [],
   "source": [
    "# Crea y almacena los fragmentos en una base de datos de vectores\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OnigeBv8WI0V",
    "outputId": "d7aa7e88-b0bb-4a02-d2f9-44e964bf9b63"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=821a006c-0a6c-46df-ade3-a0e71ec6c865,id=821a006c-0a6c-46df-ade3-a0e71ec6c865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=821a006c-0a6c-46df-ade3-a0e71ec6c865,id=821a006c-0a6c-46df-ade3-a0e71ec6c865\n"
     ]
    }
   ],
   "source": [
    "# Configura el recuperador y realiza una consulta de ejemplo\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "\n",
    "# Muestra la cantidad de documentos recuperados\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LwZt1L98WPYc",
    "outputId": "d067d7e2-9007-4871-cf3c-3e09659ea857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"
     ]
    }
   ],
   "source": [
    "# Imprime el contenido del primer documento recuperado\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P28MOkNFWSKL"
   },
   "outputs": [],
   "source": [
    "# Actualiza nuevamente la biblioteca langchain-openai\n",
    "%pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVtVnqjpWTx4"
   },
   "outputs": [],
   "source": [
    "# Configura OpenAI nuevamente y crea el modelo GPT-4 mini\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V-IXvUAjWVxy",
    "outputId": "c1b4c02d-05e2-4c6a-d78f-aa11995c0410"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=9e1459e7-a56d-4228-b4be-791737ceb9e4,id=9e1459e7-a56d-4228-b4be-791737ceb9e4\n"
     ]
    }
   ],
   "source": [
    "# Configura y ejecuta un prompt de ejemplo usando el prompt hub\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "\n",
    "example_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYerXnNrWcL2",
    "outputId": "3b855a0c-3170-4e98-cbd5-fd165e3d80fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filler question \n",
      "Context: filler context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# Imprime el contenido del mensaje de ejemplo\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejN4AdfoWggG",
    "outputId": "e4209609-5625-4de4-f722-dfbdc0f2a7b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=968da2a5-a0a7-4fe6-9d24-4515e16b0324; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=1029ccfd-07fe-4684-b23d-15e4e3ef9d7d; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=117cb86c-ddb2-49d8-babe-943aea4cdcfe; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=43de5144-d312-4eee-9433-0d41628516e3; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=107ec05b-c4f7-46b6-b41a-a4b42551e40d; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=88858874-4281-4ec2-b71e-81437ab39311\n",
      "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=4851b593-91d2-42bc-93ec-b22187d73692; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=3c4bcf65-6943-4c64-9138-797dfeb8db64; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=f9c75b94-9ebe-40d0-a0b1-92457589223f; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=09b25ab6-394b-416f-8b5c-22bd02747ea3; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=638ebf8d-d7fc-4ac4-8937-9f5dd57a01a0; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=bd4452d6-d386-4098-b1a3-b012940737e7; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=d9bdc270-7cbd-4573-beb1-7f036ad828bd; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=5a0e8730-35a7-4c1c-8212-e7b202b83fa0; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=43de5144-d312-4eee-9433-0d41628516e3; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=88858874-4281-4ec2-b71e-81437ab39311; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=1029ccfd-07fe-4684-b23d-15e4e3ef9d7d; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=117cb86c-ddb2-49d8-babe-943aea4cdcfe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is the process of breaking down a complicated task into smaller, more manageable steps. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) can be used to enhance model performance by facilitating step-by-step reasoning and exploring multiple possibilities at each step. This approach helps simplify complex tasks and provides insight into the model's reasoning process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=741bd658-fc64-4461-8c35-ba649267e9a3; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=f9c75b94-9ebe-40d0-a0b1-92457589223f; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=5a0e8730-35a7-4c1c-8212-e7b202b83fa0; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=968da2a5-a0a7-4fe6-9d24-4515e16b0324; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=4851b593-91d2-42bc-93ec-b22187d73692; trace=968da2a5-a0a7-4fe6-9d24-4515e16b0324,id=3c4bcf65-6943-4c64-9138-797dfeb8db64\n"
     ]
    }
   ],
   "source": [
    "# Configura una cadena RAG con plantillas y respuesta a preguntas\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OPKbUjQkWjBf",
    "outputId": "c456c5bd-f9e8-445e-a02b-9e478c520468"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}\n",
      "\n",
      "page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}\n",
      "\n",
      "page_content='Resources:\n",
      "1. Internet access for searches and information gathering.\n",
      "2. Long Term memory management.\n",
      "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
      "4. File output.\n",
      "\n",
      "Performance Evaluation:\n",
      "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
      "2. Constructively self-criticize your big-picture behavior constantly.\n",
      "3. Reflect on past decisions and strategies to refine your approach.\n",
      "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}\n",
      "\n",
      "page_content='(3) Task execution: Expert models execute on the specific tasks and log results.\n",
      "Instruction:\n",
      "\n",
      "With the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373}\n",
      "\n",
      "page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17804}\n",
      "\n",
      "page_content='Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\n",
      "The system comprises of 4 stages:\n",
      "(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\n",
      "Instruction:' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17414}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imprime los documentos fuente en respuesta\n",
    "for document in response[\"context\"]:\n",
    "    print(document)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fv8Ib5HlWpnT"
   },
   "outputs": [],
   "source": [
    "# Configura y ejecuta un prompt personalizado con mensaje de cortesía\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
